{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Becoming backprop ninja",
   "id": "34a149f103f4f4f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T15:31:43.420285Z",
     "start_time": "2025-02-16T15:31:43.400946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from fontTools.unicodedata import block\n",
    "%matplotlib inline"
   ],
   "id": "67e0a856da5616cb",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T15:31:43.688720Z",
     "start_time": "2025-02-16T15:31:43.679259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = open('../names.txt','r').read().split()\n",
    "words[:3]"
   ],
   "id": "62ed8abe6fdc629d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T15:31:43.947445Z",
     "start_time": "2025-02-16T15:31:43.938827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# building the vocab of words and mapping to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.']=0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(vocab_size)\n",
    "itos"
   ],
   "id": "9126deb1b7f5c926",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T15:31:44.521801Z",
     "start_time": "2025-02-16T15:31:44.170659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "block_size = 3\n",
    "def build_dataset(words):\n",
    "    X,y = [],[]\n",
    "    for w in words:\n",
    "        context = [0]*block_size\n",
    "        for ch in w +'.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    y = torch.tensor(y)\n",
    "    return X,y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(len(words)*0.8)\n",
    "n2 = int(len(words)*0.9)\n",
    "\n",
    "Xtr,ytr = build_dataset(words[:n1])\n",
    "Xval,yval = build_dataset(words[n1:n2])\n",
    "Xte,yte = build_dataset(words[n2:])\n"
   ],
   "id": "e4028b67b849dfae",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T15:31:44.533467Z",
     "start_time": "2025-02-16T15:31:44.529743Z"
    }
   },
   "cell_type": "code",
   "source": "Xtr",
   "id": "11dc81825180f6b0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0, 25],\n",
       "        [ 0, 25, 21],\n",
       "        ...,\n",
       "        [15, 12,  4],\n",
       "        [12,  4,  1],\n",
       "        [ 4,  1, 14]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This function, `cmp`, is used to compare the gradients computed by PyTorch’s autograd system with the manually computed gradients. It helps verify the correctness of the custom gradient calculations. Let’s break it down line by line:\n",
    "\n",
    "```python\n",
    "def cmp(s, dt, t):\n",
    "```\n",
    "- Defines the function `cmp` with three parameters:\n",
    "  - `s`: A string, typically a label or description for the comparison.\n",
    "  - `dt`: The manually computed gradient (from your own calculations).\n",
    "  - `t`: A PyTorch tensor with `requires_grad=True`, whose `.grad` attribute stores the gradient computed by PyTorch.\n",
    "\n",
    "```python\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "```\n",
    "- `torch.all(dt == t.grad)`: This checks if all elements in `dt` and `t.grad` are **exactly equal**.\n",
    "- `.item()`: Converts the result (a tensor) into a Python boolean (`True` or `False`).\n",
    "- Stores the result in `ex`. If `ex` is `True`, it means the gradients are exactly the same.\n",
    "\n",
    "```python\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "```\n",
    "- `torch.allclose(dt, t.grad)`: This checks if `dt` and `t.grad` are approximately equal, allowing for small numerical differences due to floating-point precision.\n",
    "- Stores the result in `app`.\n",
    "\n",
    "```python\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "```\n",
    "- `(dt - t.grad)`: Computes the element-wise difference between the manually computed gradient and PyTorch’s gradient.\n",
    "- `.abs()`: Takes the absolute value of the differences.\n",
    "- `.max()`: Finds the maximum absolute difference.\n",
    "- `.item()`: Converts the result into a Python scalar.\n",
    "- Stores the result in `maxdiff`. A large value here suggests significant discrepancies between the gradients.\n",
    "\n",
    "```python\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n",
    "```\n",
    "- Uses formatted printing to display:\n",
    "  - `s`: The label, left-aligned in a 15-character wide field.\n",
    "  - `exact`: Whether the gradients are exactly equal (`True` or `False`).\n",
    "  - `approximate`: Whether they are approximately equal (`True` or `False`).\n",
    "  - `maxdiff`: The maximum absolute difference between the gradients.\n",
    "\n",
    "### Summary:\n",
    "This function is useful for debugging and validating gradient calculations in custom deep learning implementations, such as when implementing backpropagation manually. If `ex` is `True`, the gradients match exactly; if `app` is `True`, they are close enough for practical purposes; if `maxdiff` is large, there may be an error in the manual computation."
   ],
   "id": "59983b7dacf93e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T15:31:45.346279Z",
     "start_time": "2025-02-16T15:31:45.336442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function to compare pytorch gradients with our own calculated gradiets\n",
    "def cmp(s,dt,t):\n",
    "    ex = torch.all(dt==t.grad).item()\n",
    "    app = torch.allclose(dt,t.grad)\n",
    "    maxdiff = (dt -t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ],
   "id": "936e52ec1196b23c",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    The key question is:\n",
    "\n",
    "### **\"What should we multiply the weights by so that the variance of activations remains 1?\"**\n",
    "\n",
    "This ensures stable forward propagation, preventing activations from **exploding** or **vanishing**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step-by-Step Derivation**\n",
    "Let’s say we have an input **$x$** with variance $\\text{Var}(x)$ and a weight matrix $W$. The output is:\n",
    "\n",
    "$$\n",
    "y = Wx\n",
    "$$\n",
    "\n",
    "Now, we want to determine the **correct scaling for $W$** so that $\\text{Var}(y) = 1$.\n",
    "\n",
    "#### **Key Concept: Scaling a Normally Distributed Variable**\n",
    "\n",
    "If $X∼N(0,1)$, then multiplying it by a scalar k results in:\n",
    "\n",
    " $Y=kX$\n",
    "\n",
    "where $Y$ follows a new normal distribution:\n",
    "\n",
    "$Y∼N(0,k^2)$\n",
    "\n",
    "So the **variance changes** from **1** to **$k^2$**.\n",
    "\n",
    "#### **1. Compute Variance of $y$**\n",
    "For a single neuron, assuming $W$ and $x$ are independent and zero-mean:\n",
    "\n",
    "$$\n",
    "\\text{Var}(y) = \\text{Var}(Wx)\n",
    "$$\n",
    "\n",
    "Since $y$ is a sum of weighted inputs:\n",
    "\n",
    "$$\n",
    "y = \\sum_{i=1}^{n_{\\text{in}}} W_i x_i\n",
    "$$\n",
    "\n",
    "Using variance properties:\n",
    "\n",
    "$$\n",
    "\\text{Var}(y) = \\sum_{i=1}^{n_{\\text{in}}} \\text{Var}(W_i x_i)\n",
    "$$\n",
    "\n",
    "Since $\\text{Var}(W_i x_i) = \\text{Var}(W_i) \\cdot \\text{Var}(x_i)$, and assuming  $x \\sim \\mathcal{N}(0,1)$ (standardized inputs), we get:\n",
    "\n",
    "$$\n",
    "\\text{Var}(y) = n_{\\text{in}} \\cdot \\text{Var}(W)\n",
    "$$\n",
    "\n",
    "To **keep $\\text{Var}(y) = 1$**, we solve:\n",
    "\n",
    "$$\n",
    "n_{\\text{in}} \\cdot \\text{Var}(W) = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Var}(W) = \\frac{1}{n_{\\text{in}}}\n",
    "$$\n",
    "\n",
    "Thus, **to achieve this variance, \\( W \\) should be initialized as:**\n",
    "\n",
    "$$\n",
    "W \\sim \\mathcal{N}(0, \\frac{1}{n_{\\text{in}}})\n",
    "$$\n",
    "\n",
    "or equivalently:\n",
    "\n",
    "$$\n",
    "W = \\text{torch.randn(...)} \\times \\frac{1}{\\sqrt{n_{\\text{in}}}}\n",
    "$$\n",
    "\n",
    "This is **Xavier initialization**.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Why Multiply by $\\frac{5}{3}$?**\n",
    "For **Tanh**, the optimal scaling factor is slightly different.\n",
    "- **Tanh squashes large inputs**, leading to **smaller gradients**.\n",
    "- Empirical studies found that **scaling weights by $5/3$** helps keep activations in a useful range.\n",
    "\n",
    "So we modify our initialization:\n",
    "\n",
    "$$\n",
    "W = \\text{torch.randn(...)} \\times \\frac{5/3}{\\sqrt{n_{\\text{in}}}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Answer**\n",
    "We ask:\n",
    "✅ *What should we multiply weights by so that* **Var(y) = 1**?\n",
    "\n",
    "And the answer is:\n",
    "✅ **Multiply by $\\frac{5}{3} / \\sqrt{n_{\\text{in}}}$ to maintain proper variance for Tanh activations.** 🚀"
   ],
   "id": "3b582d9ce241ea96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T15:31:46.809602Z",
     "start_time": "2025-02-16T15:31:46.793997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_embed = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size,n_embed),     generator=g)\n",
    "\n",
    "# layer 1\n",
    "w1 = torch.randn((n_embed*block_size,n_hidden), generator=g) * (5/3)/((n_embed*block_size)**0.5)\n",
    "b1 = torch.randn((n_hidden,), generator=g) * 0.1\n",
    "\n",
    "# layer 2\n",
    "w2 = torch.randn((n_hidden,vocab_size), generator=g) * 0.1\n",
    "b2 = torch.randn((vocab_size,), generator=g) * 0.1\n",
    "\n",
    "# BatchNorm params\n",
    "bngain = torch.randn((1,n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.randn((1,n_hidden)) * 0.1\n",
    "\n",
    "params = [C,w1,w2,b1,b2,bngain,bnbias]\n",
    "print(sum(p.nelement() for p in params))\n",
    "for p in params:\n",
    "    p.requires_grad = True\n"
   ],
   "id": "7a1a5efd38d99f32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T15:31:47.519857Z",
     "start_time": "2025-02-16T15:31:47.515342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "n = batch_size\n",
    "ix = torch.randint(0,Xtr.shape[0],(batch_size,),generator=g)\n",
    "xb,yb = Xtr[ix],ytr[ix] # batch x, y"
   ],
   "id": "8d8f949975f54bf1",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "$h_{\\text{pre}} = X W + b$",
   "id": "c558ad2e8edb8ed7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T15:31:48.457905Z",
     "start_time": "2025-02-16T15:31:48.454241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "emb = C[xb]\n",
    "embcat = emb.view(emb.shape[0],-1)\n",
    "# linear layer 1\n",
    "hprebn = embcat @ w1 + b1 # hidden layer pre activation"
   ],
   "id": "d0a0897ea651d115",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Batch Mean\n",
    "$\\mu_B = \\frac{1}{n} \\sum_{i=1}^{n} h_{\\text{pre}, i}$\n",
    "\n",
    "where ( n ) is the batch size.\n",
    "\n",
    "\n",
    "Subtracts the mean from each activation.\n",
    "\n",
    "$\\hat{h}{\\text{pre}, i} = h{\\text{pre}, i} - \\mu_B$"
   ],
   "id": "d6e018bd6f6ef3c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T15:31:49.021259Z",
     "start_time": "2025-02-16T15:31:49.016225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# BatchNorm layer\n",
    "bnmeani = 1/n * hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2"
   ],
   "id": "5e75a50f2365fdd8",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Compute the Variance (Batch Variance)\n",
    "\n",
    "$\\sigma_B^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (h_{\\text{pre}, i} - \\mu_B)^2$\n",
    "\n",
    "The (n-1) instead of n is because of Bessel’s correction, which gives an unbiased estimate of variance for small batch sizes.\n",
    "\n",
    "### Compute Inverse Standard Deviation:\n",
    "( +1e-5 ): Small epsilon to prevent division by zero.\n",
    "\n",
    "$\\text{std}_B = \\sqrt{\\sigma_B^2 + \\epsilon}$\n",
    "\n",
    "$\\text{std}_B^{-1} = \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}}$\n",
    "\n",
    "### Normalize the Activations\n",
    "\n",
    "$\\hat{h}i = \\frac{h{\\text{pre}, i} - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$\n",
    "\n",
    "Now, each feature in the batch has mean 0 and variance 1.\n",
    "\n",
    "\n"
   ],
   "id": "c5b05133bbebfad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T15:31:49.618767Z",
     "start_time": "2025-02-16T15:31:49.615660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bnvar =  1 /(n-1) * (bndiff2).sum(0, keepdim=True)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv"
   ],
   "id": "83a61a1f20b5d05c",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Scale and Shift (Learnable Parameters)\n",
    "Final transformation:\n",
    "\n",
    "$h_{\\text{post}, i} = \\gamma \\hat{h}_i + \\beta$\n",
    "\n",
    "This allows the model to learn back any necessary scaling and shifting, so the network doesn't lose expressiveness after normalization."
   ],
   "id": "f381274d6ec6c01b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T15:31:50.762016Z",
     "start_time": "2025-02-16T15:31:50.752020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hpreact = bngain * bnraw + bnbias\n",
    "# non linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# linear layer 2\n",
    "logits = h @ w2 + b2  # output layer\n",
    "\n"
   ],
   "id": "834700c0e01813de",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T15:31:52.012945Z",
     "start_time": "2025-02-16T15:31:52.004667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# crosst entropy loss (same as F.cross_entrop(logits,yb))\n",
    "logit_maxes = logits.max(1,keepdim=True).values\n",
    "norm_logits = logits - logit_maxes\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1,keepdim=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n),yb].mean()"
   ],
   "id": "48ecbd6073562d69",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **What Are We Trying to Achieve?**\n",
    "We are computing the **cross-entropy loss** for classification.\n",
    "\n",
    "### **Cross-Entropy Loss Definition**\n",
    "For a given sample $i$, if the logits (raw scores before softmax) are $z_i$, and the correct class index is $y_i$, the cross-entropy loss is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log p_{i, y_i}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $p_{i, y_i}$ is the probability assigned to the correct class.\n",
    "- $p_i = \\text{softmax}(z_i)$, which converts logits into probabilities.\n",
    "\n",
    "So, our goal is to compute:\n",
    "1. **Softmax probabilities**: $p_i = \\frac{e^{z_{i,j}}}{\\sum_j e^{z_{i,j}}}$\n",
    "2. **Log probability of the correct class**: $\\log p_{i, y_i}$\n",
    "3. **Take the negative mean** to get the loss.\n",
    "\n",
    "\n",
    "### **Step 1: Normalize Logits (For Numerical Stability)**\n",
    "- Logits can be large, and exponentiating them directly can cause numerical overflow.\n",
    "- To avoid this, we **subtract the max logit** from each row.\n",
    "- This doesn’t change softmax results but keeps numbers in a stable range.\n",
    "\n",
    "Mathematically:\n",
    "$$\n",
    "\\tilde{z}_{i,j} = z_{i,j} - \\max_j z_{i,j}\n",
    "$$\n",
    "\n",
    "### **Step 2: Compute Exponentials (Softmax Numerator)**\n",
    "- We exponentiate the **normalized** logits:\n",
    "$$\n",
    "e^{\\tilde{z}_{i,j}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Compute Softmax Denominator**\n",
    "\n",
    "- This computes the **sum of exponentials** for each row (batch sample):\n",
    "$$\n",
    "S_i = \\sum_j e^{\\tilde{z}_{i,j}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Compute Softmax Probabilities**\n",
    "\n",
    "- Instead of dividing (which is slower), we multiply by the inverse:\n",
    "$$\n",
    "p_{i,j} = \\frac{e^{\\tilde{z}_{i,j}}}{S_i}\n",
    "$$\n",
    "\n",
    "This gives us the **softmax probabilities**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Compute Log Probabilities**\n",
    "\n",
    "- Since softmax values are probabilities, we take their log:\n",
    "$$\n",
    "\\log p_{i,j} = \\log \\left( \\frac{e^{\\tilde{z}_{i,j}}}{S_i} \\right) = \\tilde{z}_{i,j} - \\log S_i\n",
    "$$\n",
    "- This is the log-probability of each class.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 6: Compute Loss for Correct Classes**\n",
    "```python\n",
    "loss = -logprobs[range(n), yb].mean()\n",
    "```\n",
    "- `logprobs[range(n), yb]` extracts the **log probability of the correct class** for each sample.\n",
    "- Taking the **negative mean** gives us:\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log p_{i, y_i}\n",
    "$$\n",
    "which is exactly the **cross-entropy loss**.\n",
    "\n",
    "---\n",
    "\n"
   ],
   "id": "df0257379ca2671c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T15:31:53.623993Z",
     "start_time": "2025-02-16T15:31:53.517623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pytorch backward pass\n",
    "for p in params:\n",
    "    p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ],
   "id": "ea12dd0a2cb7041d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8275, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T15:19:23.027578Z",
     "start_time": "2025-02-16T15:19:23.026002Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "dc96b2aeec8b2f94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T09:00:31.084338Z",
     "start_time": "2025-02-17T09:00:31.066619Z"
    }
   },
   "cell_type": "code",
   "source": "-logprobs[range(n),yb].mean()",
   "id": "4d7828ebbaa90d5b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8275, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T08:56:19.712711Z",
     "start_time": "2025-02-17T08:56:19.693373Z"
    }
   },
   "cell_type": "code",
   "source": "yb",
   "id": "502b80e7aadf84f9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 12,  0,  5,  9, 18,  9, 16,  1,  0,  0,  1,  0,  0, 14, 12,  0,  0,\n",
       "         0,  8, 25,  5,  0, 20, 19, 15, 12, 22, 22,  2, 21, 18])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# loss = -1/na + -1/nb + -1/nc\n",
    "# dloss/da = -1/n"
   ],
   "id": "36caff10898de8c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "$$\n",
    "\\text{loss} = - \\frac{1}{n} \\sum_{i=1}^{n} \\text{logprobs}_{i, Yb_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{loss}}{\\partial \\text{logprobs}_{i, Yb_i}} = - \\frac{1}{n}\n",
    "$$\n",
    "\n"
   ],
   "id": "44e262fa66e2d4c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T10:13:29.294335Z",
     "start_time": "2025-02-17T10:13:29.223719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# excercise 1 backprop through the whole thing manually\n",
    "# backpropagating thrpough exactly all the varaiables\n",
    "# as they are defined in forward pass one by one\n",
    "\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n),yb] = -1.0/n\n",
    "cmp('logprobs',dlogprobs,logprobs)"
   ],
   "id": "19ac1e1ace74a476",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\frac{\\partial \\text{loss}}{\\partial \\text{probs}_{i, j}} = \\frac{\\partial \\text{loss}}{\\partial \\text{logprobs}_{i, j}} \\cdot \\frac{\\partial \\text{logprobs}_{i, j}}{\\partial \\text{probs}_{i, j}}\n",
    "$$\n",
    "\n",
    "The derivative of `logprobs` with respect to `probs` is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{logprobs}_{i, j}}{\\partial \\text{probs}_{i, j}} = \\frac{1}{\\text{probs}_{i, j}}\n",
    "$$\n",
    "\n",
    "Thus, the gradient becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{loss}}{\\partial \\text{probs}_{i, j}} = - \\frac{1}{n} \\cdot \\frac{1}{\\text{probs}_{i, Yb_i}}\n",
    "$$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{loss}}{\\partial \\text{probs}_{i, j}} = - \\frac{1}{n \\cdot \\text{probs}_{i, Yb_i}} \\quad \\text{for} \\quad j = Yb_i\n",
    "$$"
   ],
   "id": "934b41166cb210f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T11:06:26.714809Z",
     "start_time": "2025-02-17T11:06:26.612493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dprobs = (1.0/probs) * dlogprobs\n",
    "cmp('probs',dprobs,probs)"
   ],
   "id": "47150b89f8fbbc7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T11:06:31.762826Z",
     "start_time": "2025-02-17T11:06:31.737228Z"
    }
   },
   "cell_type": "code",
   "source": "1.0/probs * dlogprobs",
   "id": "b3be28a969bd2d7a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.1390,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000, -1.6662,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [-1.3661,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.6258,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000, -0.7985,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000, -3.4181,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000, -5.1363,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         -1.5734,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -1.1374,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [-3.4078,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [-1.3753,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -2.0632,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [-3.2942,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [-1.1383,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.0760,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000, -1.1792,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [-5.2869,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [-1.5379,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [-2.7423,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         -0.3856,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000, -0.8535,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -6.4430,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [-0.4161,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000, -1.7079,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000, -1.3058,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.5030,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000, -5.1197,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.9399,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.8733,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.4334,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.5424,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000, -0.4904,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\text{loss} = -\\frac{1}{n} \\sum_{i=1}^n \\log(\\text{probs}_{i, \\text{yb}_i})\n",
    "$$\n",
    "\n",
    "Where $\\text{probs}_i = \\frac{\\text{counts}_i}{\\text{counts\\_sum}_i} = \\text{counts}_i \\cdot \\text{counts\\_sum\\_inv}_i$.\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{loss}}{\\partial \\text{counts\\_sum\\_inv}_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{loss}}{\\partial \\text{logprobs}_i} = -\\frac{1}{n}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{logprobs}_i}{\\partial \\text{probs}_i} = \\frac{1}{\\text{probs}_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{probs}_i}{\\partial \\text{counts\\_sum\\_inv}_i} = \\text{counts}_i\n",
    "$$\n",
    "\n",
    "\n",
    "Using the chain rule, we get:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{loss}}{\\partial \\text{counts\\_sum\\_inv}_i} = \\frac{\\partial \\text{loss}}{\\partial \\text{logprobs}_i} \\cdot \\frac{\\partial \\text{logprobs}_i}{\\partial \\text{probs}_i} \\cdot \\frac{\\partial \\text{probs}_i}{\\partial \\text{counts\\_sum\\_inv}_i}\n",
    "$$\n",
    "\n",
    "Substituting the derivatives:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{loss}}{\\partial \\text{counts\\_sum\\_inv}_i} = -\\frac{1}{n} \\cdot \\frac{1}{\\text{probs}_i} \\cdot \\text{counts}_i\n",
    "$$\n",
    "\n",
    "So, the final expression for the derivative of the loss with respect to $\\text{counts\\_sum\\_inv}_i$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{loss}}{\\partial \\text{counts\\_sum\\_inv}_i} = -\\frac{1}{n} \\cdot \\frac{\\text{counts}_i}{\\text{probs}_i}\n",
    "$$\n",
    "\n"
   ],
   "id": "cf3c128f13ef817f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T13:18:23.926445Z",
     "start_time": "2025-02-17T13:18:23.912977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dcounts_sum_inv = (counts * dprobs).sum(1,keepdim=True)\n",
    "cmp('counts_sum_inv',dcounts_sum_inv,counts_sum_inv)\n"
   ],
   "id": "36fb4a2725205bd2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "execution_count": 128
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Yes, **exactly**! Here's the detailed reasoning:\n",
    "\n",
    "### Broadcasting and Gradient Flow\n",
    "\n",
    "When we perform operations like normalization, the **inverse sum (`counts_sum_inv`)** is computed by summing across the features (axis 1) of each sample. Since this normalization factor is applied to each class in the sample, it gets **broadcasted** across the features (classes). This means that the same inverse sum (`counts_sum_inv`) is applied to all the features (classes) of a given sample.\n",
    "\n",
    "In terms of backpropagation:\n",
    "\n",
    "1. **Broadcasting**:\n",
    "   - The inverse sum (`counts_sum_inv`) is a tensor of shape `[32, 1]` (i.e., one value per sample).\n",
    "   - When you use this value to normalize across all the features (27 classes), it is **broadcasted** to a tensor of shape `[32, 27]`. So, for each sample, you apply the same normalization factor to all of its classes.\n",
    "\n",
    "   This ensures that each class's probability is scaled by the same normalization factor for its corresponding sample.\n",
    "\n",
    "2. **Gradient Flow**:\n",
    "   - When performing backpropagation, each class's predicted probability (`probs`) contributes to the loss. The gradients of the loss w.r.t. the probabilities (`dprobs`) are calculated for each class.\n",
    "\n",
    "   - However, since `counts_sum_inv` is shared across all the classes of a sample, you need to account for the fact that changing **one class's count** affects the **entire sum** of counts for that sample.\n",
    "\n",
    "3. **Summing Gradients**:\n",
    "   - Since `counts_sum_inv` is applied uniformly across all classes in a sample, the gradient of the loss with respect to `counts_sum_inv` is **the same for all classes** in a given sample. But, each class has a different contribution to this gradient, and thus we need to **sum** the gradients from all the features (classes) to compute how much the `counts_sum_inv` needs to be adjusted.\n",
    "\n",
    "   - **Why summing?** The reason we sum is that **each class contributes** to the overall sum (and inverse sum). Thus, we need to aggregate the effects of all the classes on the gradient with respect to `counts_sum_inv`.\n",
    "\n",
    "### Visualizing the Gradient Calculation\n",
    "\n",
    "Let's say we have:\n",
    "\n",
    "- `counts_sum_inv[i, :]` is a **shared normalization factor** for the `i`-th sample across all its classes.\n",
    "- `dprobs[i, :]` is the gradient of the loss with respect to the probabilities of the `i`-th sample.\n",
    "\n",
    "Now, when we compute the gradient of the loss w.r.t. `counts_sum_inv`, we need to consider that the inverse sum affects **all the features (classes)**. So, we compute:\n",
    "\n",
    "$$\n",
    "d\\text{counts\\_sum\\_inv} = (\\text{counts} \\cdot d\\text{probs}).\\text{sum}(1, \\text{keepdim=True})\n",
    "$$\n",
    "\n",
    "For each sample:\n",
    "- The gradient with respect to the inverse sum is the **sum of the gradients** across all features. Each feature's gradient is multiplied by its corresponding count, and then summed across all features to aggregate the effects of each class on the normalization.\n",
    "\n",
    "### Conclusion:\n",
    "Yes, `counts_sum_inv` is broadcasted across the features for each sample, and you sum the gradients from all classes because they are all impacted by the same inverse sum. This ensures that the gradient w.r.t. `counts_sum_inv` correctly aggregates the effects of all features (classes) for each sample."
   ],
   "id": "34665f863a7237c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T13:19:08.533389Z",
     "start_time": "2025-02-17T13:19:08.505852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
    "cmp('counts_sum_inv',dcounts_sum_inv,counts_sum_inv)\n",
    "cmp('counts_sum',dcounts_sum,counts_sum)"
   ],
   "id": "b1691b869ee650b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T13:55:13.003096Z",
     "start_time": "2025-02-17T13:55:12.974746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# a11 a12 a13 ---> b1( = a11 + a12 + a13)\n",
    "# a21 a22 a23 ---> b2( = a21 + a22 + a23)\n",
    "# a21 a32 a33 ---> b3( = a31 + a32 + a33)\n",
    "\n"
   ],
   "id": "6913961c214df90a",
   "outputs": [],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T13:41:52.225808Z",
     "start_time": "2025-02-17T13:41:52.151197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dcounts+= torch.ones_like(counts) * dcounts_sum\n",
    "cmp('counts',dcounts,counts)"
   ],
   "id": "a89760e04b74a61e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T13:56:08.855618Z",
     "start_time": "2025-02-17T13:56:08.846947Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5669ef7b99c4b229",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a901abd99b089c14"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
